{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1 << 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def init(x_ptr, stride_x, stride_y, N):\n",
    "    pid_x = tl.program_id(axis=0)\n",
    "    pid_y = tl.program_id(axis=1)\n",
    "    \n",
    "    x = pid_x * 32 + tl.arange(0, 32)\n",
    "    y = pid_y * 32 + tl.arange(0, 32)\n",
    "    \n",
    "    for i in range(0, N, stride_x):\n",
    "        for j in range(0, N, stride_y):\n",
    "            idx = (i + x[:, None]) * N + (j + y[None, :])\n",
    "            mask = (i + x[:, None] < N) & (j + y[None, :] < N)\n",
    "            val = (i + x[:, None]) * (j + y[None, :])\n",
    "            tl.store(x_ptr + idx, val, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def matmul_gpu(a_ptr, b_ptr, c_ptr, N):\n",
    "    pid_x = tl.program_id(axis=0)\n",
    "    pid_y = tl.program_id(axis=1)\n",
    "    \n",
    "    x = pid_x * 32 + tl.arange(0, 32)\n",
    "    y = pid_y * 32 + tl.arange(0, 32)\n",
    "    \n",
    "    for i in range(0, N, 32*256):\n",
    "        for j in range(0, N, 32*256):\n",
    "            i_idx = i + x[:, None]\n",
    "            j_idx = j + y[None, :]\n",
    "            \n",
    "            mask = (i_idx < N) & (j_idx < N)\n",
    "            val = tl.zeros((32, 32), dtype=tl.int32)\n",
    "            \n",
    "            for k in range(0, N):\n",
    "                a_val = tl.load(a_ptr + i_idx * N + k, mask=i_idx < N, other=0)\n",
    "                b_val = tl.load(b_ptr + k * N + j_idx, mask=j_idx < N, other=0)\n",
    "                val += a_val * b_val\n",
    "            \n",
    "            tl.store(c_ptr + i_idx * N + j_idx, val, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_cpu(a_cpu, b_cpu, c_cpu):\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            a_cpu[i * N + j] = i * j\n",
    "            b_cpu[i * N + j] = i * j\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            val = 0\n",
    "            for k in range(N):\n",
    "                val += a_cpu[i * N + k] * b_cpu[k * N + j]\n",
    "            c_cpu[i * N + j] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    a = torch.zeros(N*N, dtype=torch.int32, device='cuda')\n",
    "    b = torch.zeros(N*N, dtype=torch.int32, device='cuda')\n",
    "    c = torch.zeros(N*N, dtype=torch.int32, device='cuda')\n",
    "    \n",
    "    start = time.time()\n",
    "    grid = (triton.cdiv(N, 32), triton.cdiv(N, 32))\n",
    "    init[grid](a, 32*256, 32*256, N)\n",
    "    init[grid](b, 32*256, 32*256, N)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "    matmul_gpu[grid](a, b, c, N)\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    a_cpu = np.zeros(N*N, dtype=np.int32)\n",
    "    b_cpu = np.zeros(N*N, dtype=np.int32)\n",
    "    c_cpu = np.zeros(N*N, dtype=np.int32)\n",
    "    \n",
    "    start = time.time()\n",
    "    matmul_cpu(a_cpu, b_cpu, c_cpu)\n",
    "    cpu_time = time.time() - start\n",
    "    \n",
    "    a_np = a_cpu.reshape(N, N)\n",
    "    b_np = b_cpu.reshape(N, N)\n",
    "    start = time.time()\n",
    "    c_np = np.matmul(a_np, b_np)\n",
    "    numpy_time = time.time() - start\n",
    "\n",
    "    print(f\"GPU matmul time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"CPU loop time: {cpu_time:.4f} seconds\")\n",
    "    print(f\"NumPy matmul time: {numpy_time:.4f} seconds\")\n",
    "    \n",
    "    c_gpu = c.cpu().numpy()\n",
    "    flag = False\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if c_gpu[i * N + j] != c_cpu[i * N + j]:\n",
    "                print(f\"Error in c[{i}][{j}]: CPU={c_cpu[i * N + j]}, GPU={c_gpu[i * N + j]}\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "    \n",
    "    if not flag:\n",
    "        print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_676544/3761142064.py:11: RuntimeWarning: overflow encountered in scalar add\n",
      "  val += a_cpu[i * N + k] * b_cpu[k * N + j]\n",
      "/tmp/ipykernel_676544/3761142064.py:11: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  val += a_cpu[i * N + k] * b_cpu[k * N + j]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU matmul time: 0.0006 seconds\n",
      "CPU loop time: 5.6488 seconds\n",
      "NumPy matmul time: 0.0093 seconds\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
